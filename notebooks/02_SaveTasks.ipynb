{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3635b-fc48-4e50-ac81-ceb865f32dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# These are the default AWS configurations for the Analysis Sandbox.\n",
    "# that are set in the environmnet variables. \n",
    "aws_default_config = {\n",
    "    #'AWS_NO_SIGN_REQUEST': 'YES', \n",
    "    'AWS_SECRET_ACCESS_KEY': 'fake',\n",
    "    'AWS_ACCESS_KEY_ID': 'fake',\n",
    "}\n",
    "\n",
    "# To access public bucket, need to remove the AWS credentials in \n",
    "# the environment variables or the following error will occur.\n",
    "# PermissionError: The AWS Access Key Id you provided does not exist in our records.\n",
    "\n",
    "for key in aws_default_config.keys():\n",
    "    if key in os.environ:\n",
    "        del os.environ[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba7e34-981d-4174-aff8-1313978bb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import queue\n",
    "from threading import Thread\n",
    "\n",
    "import click\n",
    "import datacube\n",
    "import fsspec\n",
    "from odc.dscache import create_cache\n",
    "from odc.dscache.apps.slurpy import EOS, qmap\n",
    "from odc.dscache.tools import (\n",
    "    bin_dataset_stream,\n",
    "    dataset_count,\n",
    "    db_connect,\n",
    "    dictionary_from_product_list,\n",
    "    mk_raw2ds,\n",
    "    ordered_dss,\n",
    "    raw_dataset_stream,\n",
    ")\n",
    "from odc.dscache.tools.tiling import parse_gridspec_with_name\n",
    "from odc.stats.model import DateTimeRange\n",
    "from tqdm import tqdm\n",
    "\n",
    "from deafrica_conflux.cli.logs import logging_setup\n",
    "from deafrica_conflux.hopper import bin_solar_day, persist\n",
    "from deafrica_conflux.io import (\n",
    "    check_dir_exists,\n",
    "    check_file_exists,\n",
    "    check_if_s3_uri,\n",
    "    find_geotiff_files,\n",
    ")\n",
    "from deafrica_conflux.text import parse_tile_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f678da9-eaf3-4811-b0f6-e0dcc650a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "# Grid name africa_{10|20|30|60}\n",
    "grid_name = \"africa_30\"\n",
    "# Datacube product to search datasets for.\n",
    "product = \"wofs_ls\"\n",
    "# Only extract datasets for a given time range,\" \"Example '2020-05--P1M' month of May 2020\n",
    "temporal_range = \"2023-03--P3M\"\n",
    "# Compression setting for zstandard 1-fast, 9+ good but slow\n",
    "complevel = 6\n",
    "# Path to the directory containing the polygons raster files.\n",
    "polygons_rasters_directory = \"s3://deafrica-waterbodies-dev/waterbodies/v0.0.2/senegal_basin/conflux/historical_extent_rasters\"\n",
    "# Regular expression for filename matching when searching for the polygons raster files.\n",
    "pattern = \".*\"\n",
    "# Overwrite existing cache file.\n",
    "overwrite = True\n",
    "# Directory to write the cache file to.\n",
    "output_directory = \"s3://deafrica-waterbodies-dev/waterbodies/v0.0.2/senegal_basin/conflux/dbs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1122e-6abb-4123-871d-96619512ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger.\n",
    "logging_setup(verbose)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ff9cb-3872-40ad-91c4-02f40610ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support pathlib Paths.\n",
    "polygons_rasters_directory = str(polygons_rasters_directory)\n",
    "output_directory = str(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91ebed-0ab8-4707-b38b-cf4eecd1feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory if it does not exist.\n",
    "is_s3 = check_if_s3_uri(output_directory)\n",
    "if is_s3:\n",
    "    fs = fsspec.filesystem(\"s3\")\n",
    "else:\n",
    "    fs = fsspec.filesystem(\"file\")\n",
    "\n",
    "if not check_dir_exists(output_directory):\n",
    "    fs.makedirs(output_directory, exist_ok=True)\n",
    "    _log.info(f\"Created directory {output_directory}\")\n",
    "\n",
    "if not check_dir_exists(polygons_rasters_directory):\n",
    "    _log.error(f\"Directory {polygons_rasters_directory} does not exist!\")\n",
    "    raise FileNotFoundError(f\"Directory {polygons_rasters_directory} does not exist!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe5c46-48ae-4e35-ae53-351a8c195338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the product\n",
    "products = [product]\n",
    "# Connect to the datacube.\n",
    "dc = datacube.Datacube()\n",
    "# Get all products.\n",
    "all_products = {p.name: p for p in dc.index.products.get_all()}\n",
    "if len(products) == 0:\n",
    "    raise ValueError(\"Have to supply at least one product\")\n",
    "else:\n",
    "    for p in products:\n",
    "        if p not in all_products:\n",
    "            raise ValueError(f\"No such product found: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf59936-e8e7-4c82-96fe-0d4962cb1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the temporal range.\n",
    "temporal_range_ = DateTimeRange(temporal_range)\n",
    "\n",
    "output_db_fn = f\"{product}_{temporal_range_.short}.db\"\n",
    "output_db_fp = os.path.join(output_directory, output_db_fn)\n",
    "\n",
    "# Check if the output file exists.\n",
    "if check_file_exists(output_db_fp):\n",
    "    if overwrite:\n",
    "        fs.delete(output_db_fp, recursive=True)\n",
    "        _log.info(f\"Deleted {output_db_fp}\")\n",
    "        # Delete the local file created before uploading to s3.\n",
    "        if is_s3:\n",
    "            if check_file_exists(output_db_fn):\n",
    "                fsspec.filesystem(\"file\").delete(output_db_fn)\n",
    "                _log.info(f\"Deleted local file created before uploading to s3 {output_db_fn}\")\n",
    "    else:\n",
    "        raise FileExistsError(f\"{output_db_fp} exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111fe65-a085-4dbf-ac3c-aca322f4413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query to find the datasets.\n",
    "query = {\"time\": (temporal_range_.start, temporal_range_.end)}\n",
    "_log.info(f\"Query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd53d5-1a37-4780-856b-3d2b8d72cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"Getting dataset counts\")\n",
    "counts = {p: dataset_count(dc.index, product=p, **query) for p in products}\n",
    "\n",
    "n_total = 0\n",
    "for p, c in counts.items():\n",
    "    _log.info(f\"..{p}: {c:8,d}\")\n",
    "    n_total += c\n",
    "\n",
    "if n_total == 0:\n",
    "    raise ValueError(\"No datasets found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5449ed-554f-46d5-84cc-1d37e8135fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"Training compression dictionary...\")\n",
    "zdict = dictionary_from_product_list(dc, products, samples_per_product=50, query=query)\n",
    "_log.info(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b917654-9fc1-41dc-9510-1f7bc23df80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_s3:\n",
    "    cache = create_cache(output_db_fn, zdict=zdict, complevel=complevel, truncate=True)\n",
    "else:\n",
    "    cache = create_cache(output_db_fp, zdict=zdict, complevel=complevel, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a1a4e-746b-4c54-ab53-8a07e016e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2ds = mk_raw2ds(all_products)\n",
    "\n",
    "def db_task(products, conn, q):\n",
    "    for p in products:\n",
    "        if len(query) == 0:\n",
    "            dss = map(raw2ds, raw_dataset_stream(p, conn))\n",
    "        else:\n",
    "            dss = ordered_dss(dc, product=p, **query)\n",
    "\n",
    "        for ds in dss:\n",
    "            q.put(ds)\n",
    "    q.put(EOS)\n",
    "\n",
    "conn = db_connect()\n",
    "q = queue.Queue(maxsize=10_000)\n",
    "db_thread = Thread(target=db_task, args=(products, conn, q))\n",
    "db_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf90d1e-697a-42f4-9783-b944e5f7858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = qmap(lambda ds: ds, q, eos_marker=EOS)\n",
    "dss = cache.tee(dss)\n",
    "\n",
    "cells = {}\n",
    "grid, gridspec = parse_gridspec_with_name(grid_name)\n",
    "cache.add_grid(gridspec, grid)\n",
    "\n",
    "cfg = dict(grid=grid)\n",
    "cache.append_info_dict(\"stats/\", dict(config=cfg))\n",
    "\n",
    "dss = bin_dataset_stream(gridspec, dss, cells, persist=persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec8a80-a979-4c39-9f5e-f529698f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = f\"Processing {n_total:8,d} {product} datasets\"\n",
    "with tqdm(dss, desc=label, total=n_total) as dss:\n",
    "    for _ in dss:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd9b13-901d-45ea-9561-12e27cfbce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the required tiles.\n",
    "_log.info(f\"Total bins: {len(cells):d}\")\n",
    "_log.info(\"Filtering bins by required tiles...\")\n",
    "geotiff_files = find_geotiff_files(path=polygons_rasters_directory, pattern=pattern, verbose=False)\n",
    "\n",
    "tiles_ids = [parse_tile_ids(file) for file in geotiff_files]\n",
    "_log.info(f\"Found {len(tiles_ids)} tiles.\")\n",
    "_log.debug(f\"Tile ids: {tiles_ids}\")\n",
    "\n",
    "# Filter cells by tile ids.\n",
    "cells = {k: v for k, v in cells.items() if k in tiles_ids}\n",
    "_log.info(f\"Total bins: {len(cells):d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05acbec8-5c1c-4e3f-8e5c-5027c95a0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"For each bin, group datasets by solar day.\")\n",
    "tasks = bin_solar_day(cells)\n",
    "_log.info(f\"Total tasks: {len(tasks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c8908-ae49-4678-af12-f3fa3aa29a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_log.info(\"Removing duplicate source uuids...\")\n",
    "# Duplicates occur when queried datasets are captured around UTC midnight\n",
    "# and around weekly boundary\n",
    "tasks = {k: set(dss) for k, dss in tasks.items()}\n",
    "tasks_uuid = {k: [ds.id for ds in dss] for k, dss in tasks.items()}\n",
    "all_ids = set()\n",
    "for k, dss in tasks_uuid.items():\n",
    "    all_ids.update(dss)\n",
    "_log.info(f\"Total of {len(all_ids):,d} unique dataset IDs after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d0aab-d8e6-4e47-b781-420fca8a50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = f\"Saving {len(tasks)} tasks to disk\"\n",
    "with tqdm(tasks_uuid.items(), desc=label, total=len(tasks_uuid)) as groups:\n",
    "    for group in groups:\n",
    "        cache.add_grid_tile(grid, group[0], group[1])\n",
    "\n",
    "db_thread.join()\n",
    "cache.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f062c-38fe-44e2-b5ab-5791e300a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_s3:\n",
    "    fs.upload(output_db_fn, output_db_fp, recursive=False)\n",
    "    fsspec.filesystem(\"file\").delete(output_db_fn)\n",
    "\n",
    "_log.info(f\"Cache file written to {output_db_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314db0b-6e04-42bc-9c92-967f20eacc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint:disable=too-many-locals\n",
    "csv_path = os.path.join(output_directory, f\"{product}_{temporal_range_.short}.csv\")\n",
    "with fs.open(csv_path, \"wt\", encoding=\"utf8\") as f:\n",
    "    f.write('\"T\",\"X\",\"Y\",\"datasets\",\"days\"\\n')\n",
    "    for p, x, y in sorted(tasks):\n",
    "        dss = tasks[(p, x, y)]\n",
    "        n_dss = len(dss)\n",
    "        n_days = len(set(ds.time.date() for ds in dss))\n",
    "        line = f'\"{p}\", {x:+05d}, {y:+05d}, {n_dss:4d}, {n_days:4d}\\n'\n",
    "        f.write(line)\n",
    "        \n",
    "_log.info(f\"Written summary to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
